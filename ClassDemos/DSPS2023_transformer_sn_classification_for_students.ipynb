{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKY7ZOn/s1Tu8xxeqfJ0rN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedhere/DSPS_FBianco/blob/main/ClassDemos/DSPS2023_transformer_sn_classification_for_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncchDMJ3l2vl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "!pip install icecream\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in data"
      ],
      "metadata": {
        "id": "4nj1jHD3pfix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_parquet(\"https://github.com/FoxFortino/SCS/raw/main/data/raw/sn_data.parquet\")"
      ],
      "metadata": {
        "id": "hlEbzdNVmT8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw"
      ],
      "metadata": {
        "id": "y8JZH7I46cvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dataframe(sn_data):\n",
        "    \"\"\"\n",
        "    Extract both metadata and flux data from a dataframe.\n",
        "    \"\"\"\n",
        "    # Extract the row indices from the dataframe. These correspond to the SN\n",
        "    # name of the spectrum at each row.\n",
        "    index = sn_data.index\n",
        "\n",
        "    # Extract the sub-dataframe that contains only the columns corresponding\n",
        "    # to flux values. We do this specifically with a regex expression that\n",
        "    # takes only the columns that start with a number.\n",
        "    df_fluxes = sn_data.filter(regex=\"\\d+\")\n",
        "    fluxes = df_fluxes.to_numpy(dtype=float)\n",
        "\n",
        "    # Extract the columns that identify the flux columns. These will also be\n",
        "    # the wavelengths at for each flux value, but as a string.\n",
        "    flux_columns = df_fluxes.columns\n",
        "    wvl = flux_columns.to_numpy(dtype=float)\n",
        "\n",
        "    # In the eveny that more non-flux columns are added to these dataframes, we\n",
        "    # find all of the columns representing the metadata (such as SN class,\n",
        "    # spectral phase, etc.) by extracting all columns apart from\n",
        "    # `flux_columns`.\n",
        "    metadata_columns = sn_data.columns.difference(flux_columns)\n",
        "    df_metadata = sn_data[metadata_columns]\n",
        "\n",
        "    return (index, wvl,\n",
        "            flux_columns, metadata_columns,\n",
        "            df_fluxes, df_metadata,\n",
        "            fluxes)\n",
        "\n",
        "index, wvl, flux_columns, metadata_columns, df_fluxes, df_metadata, fluxes = extract_dataframe(df_raw)"
      ],
      "metadata": {
        "id": "aj53ovCTmVmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize data"
      ],
      "metadata": {
        "id": "tGDd2c8C1jM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@np.vectorize\n",
        "def wavelen2rgb(nm):\n",
        "    \"\"\"\n",
        "    Converts a wavelength between 380 and 780 nm to an RGB color tuple.\n",
        "\n",
        "    Willow: This code taken from rsmith-nl/wavelength_to_rgb git repo.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "        nm : float\n",
        "            Wavelength in nanometers.\n",
        "    Returns\n",
        "    -------\n",
        "        rgb : 3-tuple\n",
        "            tuple (red, green, blue) of integers in the range 0-255.\n",
        "    \"\"\"\n",
        "\n",
        "    def adjust(color, factor):\n",
        "        if color < 0.01:\n",
        "            return 0\n",
        "        max_intensity = 255\n",
        "        gamma = 0.80\n",
        "        rv = int(round(max_intensity * (color * factor) ** gamma))\n",
        "        if rv < 0:\n",
        "            return 0\n",
        "        if rv > max_intensity:\n",
        "            return max_intensity\n",
        "        return rv\n",
        "\n",
        "    # if nm < 380 or nm > 780:\n",
        "    #     raise ValueError('wavelength out of range')\n",
        "    if nm < 380:\n",
        "        nm = 380\n",
        "    if nm > 780:\n",
        "        nm = 780\n",
        "\n",
        "    red = 0.0\n",
        "    green = 0.0\n",
        "    blue = 0.0\n",
        "    # Calculate intensities in the different wavelength bands.\n",
        "    if nm < 440:\n",
        "        red = -(nm - 440.0) / (440.0 - 380.0)\n",
        "        blue = 1.0\n",
        "    elif nm < 490:\n",
        "        green = (nm - 440.0) / (490.0 - 440.0)\n",
        "        blue = 1.0\n",
        "    elif nm < 510:\n",
        "        green = 1.0\n",
        "        blue = -(nm - 510.0) / (510.0 - 490.0)\n",
        "    elif nm < 580:\n",
        "        red = (nm - 510.0) / (580.0 - 510.0)\n",
        "        green = 1.0\n",
        "    elif nm < 645:\n",
        "        red = 1.0\n",
        "        green = -(nm - 645.0) / (645.0 - 580.0)\n",
        "    else:\n",
        "        red = 1.0\n",
        "    # Let the intensity fall off near the vision limits.\n",
        "    if nm < 420:\n",
        "        factor = 0.3 + 0.7 * (nm - 380.0) / (420.0 - 380.0)\n",
        "    elif nm < 701:\n",
        "        factor = 1.0\n",
        "    else:\n",
        "        factor = 0.3 + 0.7 * (780.0 - nm) / (780.0 - 700.0)\n",
        "    # Return the calculated values in an (R,G,B) tuple.\n",
        "    return (adjust(red, factor), adjust(green, factor), adjust(blue, factor))\n",
        "\n",
        "\n",
        "def adjust_logbins(bins, current=\"center\", new=\"leftedge\"):\n",
        "    \"\"\"\n",
        "    Redefines whether an array corresponds to bin centers or bin edges.\n",
        "\n",
        "    Assuming the bins have a constant spacing in log-space (that is:\n",
        "    ``np.diff(np.log(bins))`` is a constant array) then this function will\n",
        "    shift the bins from bin-center-defined to bin-edge-defined or vice versa.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    bins : array-like\n",
        "        One dimensional array of bin positions.\n",
        "    current : {\"center\", \"leftedge\",}, default: \"center\"\n",
        "        Whether the bins array currently defines the bin centers or left edge.\n",
        "    new : {\"center\", \"leftedge\"}, default: \"leftedge\"\n",
        "        What the returned bins array should define: the bin centers or the left\n",
        "        bin edge.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_bins : array-like\n",
        "        One dimensional array of new bin positions.\n",
        "    \"\"\"\n",
        "\n",
        "    logbin = np.log(bins)\n",
        "    d_logbin = np.mean(np.diff(logbin))\n",
        "\n",
        "    if current == \"center\" and new == \"leftedge\":\n",
        "        # diff_logbin / 2 will give the bin radius in log space. If our array\n",
        "        # denotes bin-centers, then we need to subtract the bin radius from\n",
        "        # the array so that now the array is denoting left-bin-edges.\n",
        "        # Also note we need to add one more bin in log space before we take\n",
        "        # np.diff. This is so that when we subtract arrays of the same shape\n",
        "        # in the next line.\n",
        "        bin_radii = np.diff(logbin, append=logbin[-1] + d_logbin)\n",
        "        new_logbins = logbin - bin_radii * 0.5\n",
        "        new_bins = np.exp(new_logbins)\n",
        "\n",
        "    elif current == \"leftedge\" and new == \"center\":\n",
        "        bin_widths = np.diff(logbin, append=logbin[-1] + d_logbin)\n",
        "        new_logbins = logbin + bin_widths * 0.5\n",
        "        new_bins = np.exp(new_logbins)\n",
        "\n",
        "    return new_bins\n",
        "\n",
        "\n",
        "def plot_spec(ax, wvl, flux):\n",
        "    \"\"\"\n",
        "    Plot a spectrum with appropriate colors.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    wvl : (N,) array-like\n",
        "        Array defining the wavelength bin centers of the spectrograph.\n",
        "    flux : (N,) array-like\n",
        "        Array of flux values for each wavelength bin.\n",
        "    \"\"\"\n",
        "\n",
        "    if not np.any(wvl > 7000):\n",
        "        RGB = wavelen2rgb(wvl / 10)\n",
        "        RGBA = np.array(RGB).T / 255\n",
        "    else:\n",
        "        # If there are wavelength points above 7000 angstroms, make them an\n",
        "        # RGB value corresponding to 7000 angstroms. This RGB code can't\n",
        "        # handle wavelengths not between 4000 and 7000 angstroms.\n",
        "        over7000 = np.where(wvl > 7000)[0]\n",
        "        wvl_copy = wvl.copy()\n",
        "        wvl_copy[over7000] = 7000\n",
        "        RGB = wavelen2rgb(wvl_copy / 10)\n",
        "        RGBA = np.array(RGB).T / 255\n",
        "\n",
        "    errmsg = (\n",
        "        \"flux and wvl arrays should be of same size but are \"\n",
        "        f\"{flux.size} and {wvl.size} respectively. Each flux value \"\n",
        "        \"should correspond to one wavelength bin. See docstring for \"\n",
        "        \"more info.\"\n",
        "    )\n",
        "    assert wvl.size == flux.size, errmsg\n",
        "\n",
        "    wvl_LE = adjust_logbins(wvl)\n",
        "\n",
        "    ax.plot(\n",
        "        wvl[:-1],\n",
        "        flux[:-1],\n",
        "        ls=\"\",\n",
        "        c=\"k\",\n",
        "        marker=\"\",\n",
        "        )\n",
        "    _, _, patches = ax.hist(\n",
        "        wvl_LE[:-1],\n",
        "        bins=wvl_LE,\n",
        "        weights=flux[:-1],\n",
        "        align=\"mid\",\n",
        "        )\n",
        "\n",
        "    # Each patch of the histogram (the rectangle underneath each point) gets\n",
        "    # colored according to its central wavelength.\n",
        "    for patch, color in zip(patches, RGBA):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    ax.set_xlabel(r\"Wavelength [$\\AA$]\")\n",
        "    ax.set_ylabel(r\"Normalized Flux [$F_{\\lambda}$]\")\n",
        "\n",
        "    # bounds = flux.max() + flux.max()*0.05\n",
        "    # ax.ylim((-bounds, bounds))\n",
        "\n",
        "    ax.set_ylim((0, 1.05))\n",
        "\n",
        "    ax.set_ylim((flux.min(), flux.max()))\n",
        "    ax.set_xlim((3800, 7000))\n",
        "\n",
        "    ax.tick_params(axis=\"both\", which=\"major\")\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "LhhtfOdZnTTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Pc_RN0h2hmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "3R1QCJfR5jJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preproccess_dataframe(\n",
        "    sn_data,\n",
        "    phase_range=(-20, 50),\n",
        "    ptp_range=(0.1, 100),\n",
        "    wvl_range=(4500, 7000),\n",
        "    save_path=None,\n",
        "):\n",
        "    if (save_path is not None) and (not isdir(dirname(save_path))):\n",
        "        raise FileNotFoundError(f\"Directory '{dirname(save_path)}' does not exist.\")\n",
        "\n",
        "    # The function below neatly and reproducibly extracts all of the relevant\n",
        "    # subsets of the dataframe.\n",
        "    data = extract_dataframe(sn_data)\n",
        "    index = data[0]  # SN Name for each spectrum\n",
        "    wvl0 = data[1]  # Wavelength array\n",
        "    flux0_columns = data[2]  # Columns that index the fluxes in the dataframe\n",
        "    metadata_columns = data[3]  # Columns that index the metadata\n",
        "    df_fluxes0 = data[4]  # Sub-dataframe containing only the fluxes\n",
        "    df_metadata = data[5]  # Sub-dataframe containing only the metadata\n",
        "    fluxes0 = data[6]  # Only the flux values in a numpy array\n",
        "\n",
        "    # Spectra with a spectral phase outside of `phase_range`.\n",
        "    bad_ind = sn_data[\"Spectral Phase\"] < phase_range[0]\n",
        "    bad_ind |= sn_data[\"Spectral Phase\"] > phase_range[1]\n",
        "\n",
        "    # Remove the spectra with a range that is too small or too large.\n",
        "    ptp = np.ptp(fluxes0, axis=1)\n",
        "    bad_ind |= ptp < ptp_range[0]\n",
        "    bad_ind |= ptp > ptp_range[1]\n",
        "\n",
        "    # Remove rows from fluxes0 according to bad_ind\n",
        "    fluxes0 = fluxes0[~bad_ind]\n",
        "\n",
        "    # Standardize the dataset to zero mean and standard deviation of 1.\n",
        "    flux_means = np.mean(fluxes0, axis=1)[..., None]\n",
        "    flux_stds = np.std(fluxes0, axis=1)[..., None]\n",
        "    standardized_fluxes0 = (fluxes0 - flux_means) / flux_stds\n",
        "\n",
        "    # Set all flux values outside of `wvl_range` to 0.\n",
        "    standardized_fluxes0[:, (wvl0 < wvl_range[0]) | (wvl0 > wvl_range[1])] = 0\n",
        "\n",
        "    # Set the standardized flux data into the dataframe.\n",
        "    sn_data.loc[~bad_ind, flux0_columns] = standardized_fluxes0\n",
        "\n",
        "    # Remove the rows that we have pruned above.\n",
        "    sn_data = sn_data.loc[~bad_ind]\n",
        "\n",
        "    if save_path is not None:\n",
        "        sn_data.to_parquet(save_path)\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "    return sn_data\n",
        "\n",
        "df_P = preproccess_dataframe(df_raw)"
      ],
      "metadata": {
        "id": "EzXZumlt32mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wcYLybQ526L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test split...\n",
        "\n",
        "How should we do this train-test split?"
      ],
      "metadata": {
        "id": "0411vOtL8FPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_split(x, train_frac, rng):\n",
        "    x[\"Exclude\"] = False\n",
        "    x[\"Training Set\"] = False\n",
        "\n",
        "    sn_names = x.index.unique().to_list()\n",
        "    num_supernova = len(sn_names)\n",
        "    if num_supernova == 1:\n",
        "        x[\"Exclude\"] = True\n",
        "        return x\n",
        "\n",
        "    num_train = int(np.ceil(num_supernova * train_frac))\n",
        "    if num_supernova - num_train == 0:\n",
        "        num_train -= 1\n",
        "\n",
        "    inds = rng.choice(sn_names,\n",
        "                      size=num_train,\n",
        "                      replace=False)\n",
        "    x.loc[inds, \"Training Set\"] = True\n",
        "    return x\n",
        "\n",
        "\n",
        "def split_data(\n",
        "    sn_data,\n",
        "    train_frac,\n",
        "    rng,\n",
        "    save_path_trn=None,\n",
        "    save_path_tst=None,\n",
        "):\n",
        "    if (save_path_trn is not None) and (save_path_tst is not None):\n",
        "        if not isdir(dirname(save_path_tst)):\n",
        "            raise FileNotFoundError(f\"Directory '{dirname(save_path_tst)}' does not exist.\")\n",
        "        if not isdir(dirname(save_path_trn)):\n",
        "            raise FileNotFoundError(f\"Directory '{dirname(save_path_trn)}' does not exist.\")\n",
        "\n",
        "    sn_data_split = sn_data.groupby(by=[\"SN Subtype\"],\n",
        "                                    axis=0,\n",
        "                                    group_keys=True).apply(df_split,\n",
        "                                                           train_frac,\n",
        "                                                           rng)\n",
        "    training_set = sn_data_split[\"Training Set\"] & ~sn_data_split[\"Exclude\"]\n",
        "    testing_set = ~sn_data_split[\"Training Set\"] & ~sn_data_split[\"Exclude\"]\n",
        "    sn_data_trn = sn_data_split.loc[training_set]\n",
        "    sn_data_tst = sn_data_split.loc[testing_set]\n",
        "\n",
        "    sn_data_trn.reset_index(level=\"SN Subtype\", drop=True, inplace=True)\n",
        "    sn_data_tst.reset_index(level=\"SN Subtype\", drop=True, inplace=True)\n",
        "\n",
        "    if (save_path_trn is not None) and (save_path_tst is not None):\n",
        "        sn_data_trn.to_parquet(save_path_trn)\n",
        "        print(f\"Saved: {save_path_trn}\")\n",
        "        sn_data_tst.to_parquet(save_path_tst)\n",
        "        print(f\"Saved: {save_path_tst}\")\n",
        "\n",
        "    return sn_data_trn, sn_data_tst\n",
        "\n",
        "rng = np.random.RandomState(1415)\n",
        "df_trn, df_tst = split_data(df_P, 0.60, rng)"
      ],
      "metadata": {
        "id": "x4NOoNAR7IqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_trn"
      ],
      "metadata": {
        "id": "ZyK4UGI4AQFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tst"
      ],
      "metadata": {
        "id": "eKeuv21wAQ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the training and testing data for use"
      ],
      "metadata": {
        "id": "VcgDWcyaBz2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(df):\n",
        "    data = extract_dataframe(df)\n",
        "    X = data[6]\n",
        "    Y = data[5][\"SN Subtype ID\"].to_numpy(dtype=int)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    num_wvl = X.shape[1]\n",
        "    num_classes = np.unique(Y).size\n",
        "\n",
        "    Y_OH = to_categorical(Y, num_classes=num_classes)\n",
        "\n",
        "    return X, Y_OH, N, num_wvl, num_classes\n",
        "\n",
        "Xtrn, Ytrn, num_trn, num_wvl, num_classes = extract(df_trn)\n",
        "Xtst, Ytst, num_tst, num_wvl, num_classes = extract(df_tst)\n",
        "\n",
        "Xtrn.shape, Ytrn.shape, Xtst.shape, Ytst.shape, num_classes"
      ],
      "metadata": {
        "id": "36PCEGqrBvF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dim(X, swap=False):\n",
        "    X = X[..., None]\n",
        "    if swap:\n",
        "        X = np.swapaxes(X, 1, 2)\n",
        "    return X\n",
        "\n",
        "Xtrn = add_dim(Xtrn, swap=True)\n",
        "Xtst = add_dim(Xtst, swap=True)\n",
        "\n",
        "Xtrn.shape, Ytrn.shape"
      ],
      "metadata": {
        "id": "X2WAC-NECFkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the model\n",
        "\n",
        "What we want:\n",
        "- Input layer\n",
        "- One transformer encoder block such as in Vaswani et al. 2017\n",
        "- Densely connected feed-forward classification ehad\n",
        "- Output layer"
      ],
      "metadata": {
        "id": "tpEkxBdcA_T4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lex9_r52C9CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the model with loss function, metrics, and optimizer"
      ],
      "metadata": {
        "id": "1CJgf2GfFdSK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OsURpOakFUwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define callbacks"
      ],
      "metadata": {
        "id": "IfEYkhsuF8xe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TO-vpZexFuVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "zojPpfBdGV_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlXhzcFQGVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot loss curves"
      ],
      "metadata": {
        "id": "w1kbDn8cHLAV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fByMOD0HBwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize performance"
      ],
      "metadata": {
        "id": "p3xC2P8-HbRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ptrn = model.predict(Xtrn)\n",
        "Ptst = model.predict(Xtst)\n",
        "\n",
        "Ptrn_flat = np.argmax(Ptrn, axis=1)\n",
        "Ptst_flat = np.argmax(Ptst, axis=1)\n",
        "\n",
        "Ytrn_flat = np.argmax(Ytrn, axis=1)\n",
        "Ytst_flat = np.argmax(Ytst, axis=1)\n",
        "\n",
        "CMtrn = confusion_matrix(Ytrn_flat, Ptrn_flat)\n",
        "CMtst = confusion_matrix(Ytst_flat, Ptst_flat)"
      ],
      "metadata": {
        "id": "NjQInoGIIPmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SN_Stypes_str = np.array(\n",
        "    [\"Ia-norm\", \"Ia-91T\", \"Ia-91bg\", \"Ia-csm\", \"Iax\", \"Ia-pec\",\n",
        "     \"Ib-norm\", \"Ibn\", \"IIb\", \"Ib-pec\",\n",
        "     \"Ic-norm\", \"Ic-broad\", \"Ic-pec\",\n",
        "     \"IIP\", \"IIL\", \"IIn\"])\n",
        "\n",
        "\n",
        "def plot_cm(\n",
        "    cm, classes, title, normalize=True, figsize=(9, 7), fontsize_offset=9, savepath=None\n",
        "):\n",
        "    \"\"\"Normalize confusion matrix and set image parameters\"\"\"\n",
        "    cm = cm.astype(\"float\") / np.nansum(cm, axis=1)[:, np.newaxis]\n",
        "    off_diag = ~np.eye(cm.shape[0], dtype=bool)\n",
        "    cm[off_diag] *= -1\n",
        "    vmin, vmax = -1, 1\n",
        "    cmap = \"RdBu\"\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    plt.imshow(cm, interpolation=\"None\", cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "\n",
        "    plt.title(f\"{title}\")\n",
        "\n",
        "    cb = plt.colorbar()\n",
        "    cb.ax.tick_params(labelsize=23 - fontsize_offset)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15 - fontsize_offset)\n",
        "    plt.yticks(tick_marks, classes, fontsize=15 - fontsize_offset)\n",
        "\n",
        "    fmt = \".2f\" if normalize else \"d\"\n",
        "    thresh = cm.max() / 2.0\n",
        "    import itertools\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(\n",
        "            j,\n",
        "            i,\n",
        "            format(abs(cm[i, j]), fmt),\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if abs(cm[i, j]) > thresh else \"black\",\n",
        "            fontsize=18 - fontsize_offset,\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel(\"True label\", fontsize=26 - fontsize_offset)\n",
        "    plt.xlabel(\"Predicted label\", fontsize=26 - fontsize_offset)\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig(savepath)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "56Zgvr6DHlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(CMtst, SN_Stypes_str, \"Testing Set\", normalize=True)"
      ],
      "metadata": {
        "id": "qgUHkkTAHzEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(CMtrn, SN_Stypes_str, \"Training Set\", normalize=True)"
      ],
      "metadata": {
        "id": "0COB24oAIKcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}